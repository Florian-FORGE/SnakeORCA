import pandas as pd
import os

# Helper to resolve paths
def resolve_path(path, base):
    return path if os.path.isabs(path) else os.path.join(base, path)

# --- Load config and experiment table ---
print(config["experiments"])
experiment_file = resolve_path(config["experiments"], workflow.basedir)
output_repo = config["output_repo"]

experiments = pd.read_csv(experiment_file, sep='\t', dtype={"expe": str}).set_index("expe", drop=False)
expandable_abs_to_rel = os.path.join(output_repo, "{expe}", "abs_to_rel.log")
l_expe = experiments.index.tolist()
l_names = experiments["names"]
workingdir = resolve_path(config['workdir'], workflow.basedir)
workdir: workingdir

comp_expe_names = ",".join([f"{row['expe']}:{row['names']}" for _, row in experiments.iterrows()])

last_file = resolve_path(f'{config["analysis_path_comp"]}/{config["output_file_comp"]}', workingdir)

# --- Helper functions ---
def mutable(wildcards=None):
    successful = []
    for expe in l_expe:
        try:
            ckpt_output = checkpoints.abs_to_rel.get(expe=expe).output[1]
            if os.path.exists(ckpt_output):
                successful.append(expe)
        except Exception:
            pass
    expandable=f"{output_repo}" + "/{expe}/genome/reference/relative_position_mutations.bed"
    return expand(f"{expandable}", expe=successful)

def valid_expe(wildcards=None):
    # Get the output file from the check_bed checkpoint
    check_bed_output = checkpoints.check_bed.get().output[0]
    valid_experiments = []
    with open(check_bed_output) as f:
        for line in f:
            if line.strip():
                expe_name = line.split()[1].split('/')[2]
                valid_experiments.append(expe_name)
    expandable=f"{output_repo}" + "/{expe}/analysis.log"
    return expand(f"{expandable}", expe=valid_experiments)

def is_int_castable(val):
    try:
        int(val)
        return True
    except (ValueError, TypeError):
        return False

def comparable(wildcards=None):
    # Get the output file from the check_bed checkpoint
    check_bed_output = checkpoints.check_bed.get().output[0]
    valid_experiments = []
    with open(check_bed_output) as f:
        for line in f:
            if line.strip():
                expe_name = line.split()[1].split('/')[2]
                valid_experiments.append(expe_name)
    expandable=f"{output_repo}" + "/{expe}/prediction.log"
    return expand(f"{expandable}", expe=valid_experiments)

# --- Rules ---

rule all:
    input:
        f"{output_repo}" + "/done.log"


rule build:
    input:
        expand(f"{expandable_abs_to_rel}", expe=experiments.index),
        f"{output_repo}" + "/list_valid_bed.txt",
    output:
        f"{output_repo}" + "/build.log"
    params:
        len_expe=len(experiments.index),
        wdir=config["output_repo"]
    shell:
        """
        echo 'Building experience with {params.len_expe} experiments' > {output}
        echo 'Valid experiments: {input}' >> {output}
        """

checkpoint abs_to_rel:
    input: 
        genome=resolve_path(config["genome"], workflow.basedir),
        mutations=lambda wildcards: resolve_path(experiments.loc[wildcards.expe]["mutations"], workflow.basedir)
    output:
        f"{output_repo}" + "/{expe}/abs_to_rel.log",
        f"{output_repo}" + "/{expe}/genome/reference/relative_position_mutations.bed"
    params:
        outdir=f"{output_repo}" + "/{expe}/genome",
        region=config["region"]
    shell:
        """
        absolute_to_relative \
               --bed {input.mutations} \
               --fasta {input.genome} \
               --mut_path {params.outdir} \
               --region {params.region}
        """

checkpoint check_bed:
    input:
        expand(f"{expandable_abs_to_rel}", expe=experiments.index)
    
    output:
        f"{output_repo}" + "/list_valid_bed.txt"
    
    params:
        input=lambda wildcards: " ".join(mutable()),

    shell:
        """
        for repo in {params.input}; do wc -l "$repo"; done | grep -v "^0 " > {output}
        """


rule valid:
    input:
        f"{output_repo}" + "/build.log",
        valid_expe,
        last_file
    
    output:
        f"{output_repo}" + "/valid_expe.log"
    
    params:
        len_expe=len(experiments.index),
        valid=valid_expe,
        len_valid=lambda wildcards: len(wildcards),
        wdir=config["output_repo"]
    
    shell:
        """
        echo 'Mutable experiments: {params.valid}' > {output}
        echo 'Hence {params.len_valid}/{params.len_expe} were mutable' >> {output}
        """


rule launch_analysis:
    input:
        f"{output_repo}" + "/valid_expe.log"
    output:
        f"{output_repo}" + "/done.log"
    params:
        wdir=config["output_repo"]
    shell:
        """
        touch {output}
        """

rule mutate:
    input:
        rel_log=f"{output_repo}" + "/{expe}/abs_to_rel.log"
    output:
        f"{output_repo}" + "/{expe}/mutate.log"
    params:
        outdir=f"{output_repo}" + "/{expe}/genome",
        nb_random=lambda wildcards: (
                        experiments.loc[wildcards.expe]["nb_random"]
                        if (
                            "nb_random" in experiments.columns and
                            experiments.loc[wildcards.expe]["nb_random"] not in [None, "None", ""]
                        )
                        else config["nb_random"]
                    ),
        distrib=config["distrib"],
        muttype=lambda wildcards: (
                    experiments.loc[wildcards.expe]["muttype"]
                    if (
                        "muttype" in experiments.columns and
                        experiments.loc[wildcards.expe]["muttype"] not in [None, "None", ""]
                    )
                    else config["muttype"]
                ),
        binsize_permut = lambda wildcards: (
                                 int(experiments.loc[wildcards.expe]["binsize_permut"])
                                 if (
                                     "binsize_permut" in experiments.columns and
                                     is_int_castable(experiments.loc[wildcards.expe]["binsize_permut"])
                                 )
                                 else 0
                             )
    shell:
        """
        mutate_and_rdm \
               --abs_to_rel_log_path {input.rel_log} \
               --mut_path {params.outdir} \
               --muttype {params.muttype} \
               --nb_random {params.nb_random} \
               --distrib {params.distrib} \
               --binsize_permut {params.binsize_permut}
        """

rule predict:
    input:
        mut_log=f"{output_repo}" + "/{expe}/mutate.log", 
        rel_log=f"{output_repo}" + "/{expe}/abs_to_rel.log"
    output:
        f"{output_repo}" + "/{expe}/prediction.log"
    params:
        pred_prefix="{expe}",
        resol_model=config["resol_model"],
        mpos=config["mpos"],
        pred_path=f"{output_repo}" + "/{expe}/predictions",
        builder_path=f"{output_repo}" + "/{expe}/matrices_builder", 
        no_cuda=config["no_cuda"],
        gpu_mem = config["gpu_mem"]
    resources:
        predict_slots=1
    benchmark:
        "benchmarks/shuf_bins_106_138Mb/predict/{expe}.tsv"
    shell:
        """
        predict_and_run_descript \
               --pred_prefix {params.pred_prefix} \
               --resol_model {params.resol_model} \
               --mutate_log_path {input.mut_log} \
               --mpos {params.mpos} \
               --pred_path {params.pred_path} \
               --abs_to_rel_log_path {input.rel_log} \
               --builder_path {params.builder_path} \
               --no_cuda {params.no_cuda}
        """

rule analysis:
    input:
        predict_log=f"{output_repo}" + "/{expe}/prediction.log"        
    output:
        f"{output_repo}" + "/{expe}/analysis.log"
    params:
        analysis_path=f"{output_repo}" + "/{expe}/analysis",
        score_types=config["score_types"],
        merged_by=config["merged_by"],
        resol=config["resol"],
        show_rdm=config["show_rdm"],
        expe_descrip=lambda wildcards: experiments.loc[wildcards.expe]["expe_description"]
    shell:
        """
        preliminary_analysis \
               --expe_descrip '{params.expe_descrip}' \
               --prediction_log_path {input.predict_log} \
               --analysis_path {params.analysis_path} \
               --l_score_types {params.score_types} \
               --merged_by {params.merged_by} \
               --l_resol {params.resol} \
               --show_rdm {params.show_rdm}
        """


rule compare:
    input:
        comparable
    output:
        last_file
    params:
        descrip=f'"{config["descrip_comp"]}"',
        data_file=config["data_file_comp"],
        analysis_path=config["analysis_path_comp"],
        output_file=config["output_file_comp"],
        score_types=config["score_types_comp"],
        resol=config["resol_comp"],
        wdir=config["output_repo"],
        expe_names=f'"{comp_expe_names}"',
        create_data_flag="--create_data" if bool(config["create_data_file"] == True) else "",
        rename_flag="--rename" if bool(config["rename"] == True) else ""
    shell:
        """
        multiple_mut_analysis \
               --descrip {params.descrip} \
               --data_file {params.data_file} \
               --analysis_path {params.analysis_path} \
               --output_file {params.output_file} \
               --score_types {params.score_types} \
               {params.create_data_flag} \
               {params.rename_flag} \
               --resol {params.resol} \
               --wdir {params.wdir} \
               --expe_names {params.expe_names}
        """

